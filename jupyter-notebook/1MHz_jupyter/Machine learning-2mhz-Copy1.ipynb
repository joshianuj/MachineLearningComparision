{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# data = pd.read_csv('../Documents/data_set/New_Data_Set/1.14MHz Data/Result/Features/Overall/car_human_pillar_wall_overall.csv')\n",
    "car_data = pd.read_csv('../../data/features/114KHz/car.csv')\n",
    "human_data = pd.read_csv('../../data/features/114KHz/human.csv')\n",
    "pillar_data = pd.read_csv('../../data/features/114KHz/pillar.csv')\n",
    "wall_data = pd.read_csv('../../data/features/114KHz/wall.csv')\n",
    "motorbike_data = pd.read_csv('../../data/features/114KHz/motorbike.csv')\n",
    "cycle_data = pd.read_csv('../../data/features/114KHz/cycle.csv')\n",
    "\n",
    "human = human_data.loc[human_data['type'] == 'HUMAN'].iloc[:,4:]\n",
    "car = car_data.loc[car_data['type'] == 'CAR'].iloc[:,4:]\n",
    "pillar = pillar_data.loc[pillar_data['type'] == 'PILLAR'].iloc[:,4:]\n",
    "wall = wall_data.loc[wall_data['type'] == 'WALL'].iloc[:,4:]\n",
    "motorbike = motorbike_data.loc[motorbike_data['type'] == 'MOTORBIKE'].iloc[:,4:]\n",
    "cycle = cycle_data.loc[cycle_data['type'] == 'CYCLE'].iloc[:,4:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10231, 90), (12080, 90), (9568, 90), (5067, 90))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human_label = ['HUMAN']*human.shape[0]\n",
    "car_label = ['CAR']*car.shape[0]\n",
    "wall_label = ['WALL']*wall.shape[0]\n",
    "pillar_label = ['PILLAR']*pillar.shape[0]\n",
    "non_human_label = ['NON_HUMAN']*(car.shape[0] +wall.shape[0] + pillar.shape[0])\n",
    "human.shape, car.shape, pillar.shape, wall.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "def custom_normalization(X_set):\n",
    "    new_X_set = []\n",
    "    for X in X_set:\n",
    "        min = np.min(X)\n",
    "        max = np.max(X)\n",
    "        value = max - min\n",
    "        data_set = []\n",
    "        for data in X:\n",
    "           data_set.append(((data - min) / value) + 0)\n",
    "        new_X_set.append(data_set)\n",
    "    return new_X_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = human_label + non_human_label\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = human.values.tolist() + car.values.tolist() + wall.values.tolist() + pillar.values.tolist()\n",
    "X_normalized = custom_normalization(data)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_normalized, label, test_size=0.4, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "def create_confusion_matrix(y_test, result, labels = ['HUMAN', 'NON_HUMAN']):\n",
    "    cm = confusion_matrix(y_test, result)\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt     \n",
    "    sum = np.sum(cm, axis=1)\n",
    "    score = accuracy_score(y_test, result)\n",
    "    print(score)\n",
    "    from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "    \n",
    "    precision_CLASS_A = round(precision_score(y_test, result, average='binary',pos_label=labels[0]),2)\n",
    "    precision_CLASS_B = round(precision_score(y_test, result, average='binary',pos_label=labels[1]),2)\n",
    "    recall_CLASS_A = round(recall_score(y_test, result, average='binary',pos_label=labels[0]),2)\n",
    "    recall_CLASS_B = round(recall_score(y_test, result, average='binary',pos_label=labels[1]),2)\n",
    "    f1_CLASS_A = round(f1_score(y_test, result, average='binary',pos_label=labels[0]),2)\n",
    "    f1_CLASS_B = round(f1_score(y_test, result, average='binary',pos_label=labels[1]),2)\n",
    "    f1_average = round((f1_CLASS_A + f1_CLASS_B)/2, 2);\n",
    "    print('Precision: Class A',precision_CLASS_A)\n",
    "    print('Precision: Class B',precision_CLASS_B)\n",
    "    print('Recall: Class A',recall_CLASS_A)\n",
    "    print('Recall: Class B',recall_CLASS_B)\n",
    "    print('F1-Score: Class A',f1_CLASS_A)\n",
    "    print('F1-Score: Class B',f1_CLASS_B)\n",
    "    print('Average F1-score:', f1_average)\n",
    "        \n",
    "    cm_new = np.append(cm[0], recall_CLASS_A)\n",
    "    cm_new2 = np.append(cm[1], recall_CLASS_B)\n",
    "    cm_new3 = np.array([precision_CLASS_A, precision_CLASS_B, score])\n",
    "    cm = np.array([cm_new,cm_new2,cm_new3])\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10,10))\n",
    "    sns.heatmap(cm, annot=True, ax = ax,linewidths=.5,fmt='g',cmap=\"Reds\"); #annot=True to annotate cells\n",
    "\n",
    "    # labels, title and ticks\n",
    "    ax.set_xlabel('Predicted labels');\n",
    "    ax.set_ylabel('True labels'); \n",
    "    ax.set_title('Multilayer Perceptron'); \n",
    "    counter = 0\n",
    "    for i in range(0,2):\n",
    "        for j in range(0,3):\n",
    "            percentage = cm[i][j]/sum[i]\n",
    "            t = ax.texts[counter]\n",
    "            if j == 2:\n",
    "                t.set_text(str(cm[i][j]))\n",
    "            else:\n",
    "                t.set_text(str(cm[i][j]) + '\\n' + str(round(percentage*100,2)) + \" %\")\n",
    "            counter = counter + 1\n",
    "    ax.xaxis.set_ticklabels(labels)\n",
    "    ax.yaxis.set_ticklabels(labels);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.74473345\n",
      "Iteration 2, loss = 0.63030314\n",
      "Iteration 3, loss = 0.60335005\n",
      "Iteration 4, loss = 0.58840015\n",
      "Iteration 5, loss = 0.58071643\n",
      "Iteration 6, loss = 0.57691719\n",
      "Iteration 7, loss = 0.57460723\n",
      "Iteration 8, loss = 0.57265441\n",
      "Iteration 9, loss = 0.57070007\n",
      "Iteration 10, loss = 0.56884876\n",
      "Iteration 11, loss = 0.56661935\n",
      "Iteration 12, loss = 0.56233181\n",
      "Iteration 13, loss = 0.55795526\n",
      "Iteration 14, loss = 0.55475622\n",
      "Iteration 15, loss = 0.55174773\n",
      "Iteration 16, loss = 0.54868459\n",
      "Iteration 17, loss = 0.54549781\n",
      "Iteration 18, loss = 0.54215044\n",
      "Iteration 19, loss = 0.53877208\n",
      "Iteration 20, loss = 0.53523035\n",
      "Iteration 21, loss = 0.53160024\n",
      "Iteration 22, loss = 0.52780958\n",
      "Iteration 23, loss = 0.52393990\n",
      "Iteration 24, loss = 0.51997203\n",
      "Iteration 25, loss = 0.51586130\n",
      "Iteration 26, loss = 0.51175240\n",
      "Iteration 27, loss = 0.50751178\n",
      "Iteration 28, loss = 0.50317634\n",
      "Iteration 29, loss = 0.49890048\n",
      "Iteration 30, loss = 0.49458095\n",
      "Iteration 31, loss = 0.49025948\n",
      "Iteration 32, loss = 0.48601019\n",
      "Iteration 33, loss = 0.48179987\n",
      "Iteration 34, loss = 0.47765907\n",
      "Iteration 35, loss = 0.47360592\n",
      "Iteration 36, loss = 0.46969709\n",
      "Iteration 37, loss = 0.46576763\n",
      "Iteration 38, loss = 0.46212035\n",
      "Iteration 39, loss = 0.45846073\n",
      "Iteration 40, loss = 0.45494676\n",
      "Iteration 41, loss = 0.45157070\n",
      "Iteration 42, loss = 0.44823740\n",
      "Iteration 43, loss = 0.44518478\n",
      "Iteration 44, loss = 0.44213150\n",
      "Iteration 45, loss = 0.43930673\n",
      "Iteration 46, loss = 0.43651285\n",
      "Iteration 47, loss = 0.43399602\n",
      "Iteration 48, loss = 0.43140743\n",
      "Iteration 49, loss = 0.42902033\n",
      "Iteration 50, loss = 0.42676268\n",
      "Iteration 51, loss = 0.42457869\n",
      "Iteration 52, loss = 0.42251740\n",
      "Iteration 53, loss = 0.42051932\n",
      "Iteration 54, loss = 0.41882900\n",
      "Iteration 55, loss = 0.41688830\n",
      "Iteration 56, loss = 0.41521260\n",
      "Iteration 57, loss = 0.41359988\n",
      "Iteration 58, loss = 0.41204102\n",
      "Iteration 59, loss = 0.41066762\n",
      "Iteration 60, loss = 0.40923884\n",
      "Iteration 61, loss = 0.40789039\n",
      "Iteration 62, loss = 0.40659644\n",
      "Iteration 63, loss = 0.40532944\n",
      "Iteration 64, loss = 0.40423380\n",
      "Iteration 65, loss = 0.40311239\n",
      "Iteration 66, loss = 0.40207371\n",
      "Iteration 67, loss = 0.40107534\n",
      "Iteration 68, loss = 0.39998822\n",
      "Iteration 69, loss = 0.39907471\n",
      "Iteration 70, loss = 0.39817942\n",
      "Iteration 71, loss = 0.39734456\n",
      "Iteration 72, loss = 0.39644838\n",
      "Iteration 73, loss = 0.39556881\n",
      "Iteration 74, loss = 0.39492151\n",
      "Iteration 75, loss = 0.39416619\n",
      "Iteration 76, loss = 0.39320534\n",
      "Iteration 77, loss = 0.39249960\n",
      "Iteration 78, loss = 0.39177923\n",
      "Iteration 79, loss = 0.39107591\n",
      "Iteration 80, loss = 0.39038199\n",
      "Iteration 81, loss = 0.38963808\n",
      "Iteration 82, loss = 0.38898675\n",
      "Iteration 83, loss = 0.38836567\n",
      "Iteration 84, loss = 0.38777534\n",
      "Iteration 85, loss = 0.38719580\n",
      "Iteration 86, loss = 0.38650525\n",
      "Iteration 87, loss = 0.38592897\n",
      "Iteration 88, loss = 0.38546592\n",
      "Iteration 89, loss = 0.38480494\n",
      "Iteration 90, loss = 0.38426858\n",
      "Iteration 91, loss = 0.38378724\n",
      "Iteration 92, loss = 0.38318330\n",
      "Iteration 93, loss = 0.38268906\n",
      "Iteration 94, loss = 0.38226804\n",
      "Iteration 95, loss = 0.38172588\n",
      "Iteration 96, loss = 0.38126034\n",
      "Iteration 97, loss = 0.38068833\n",
      "Iteration 98, loss = 0.38025506\n",
      "Iteration 99, loss = 0.37973466\n",
      "Iteration 100, loss = 0.37926071\n",
      "Iteration 101, loss = 0.37880981\n",
      "Iteration 102, loss = 0.37838957\n",
      "Iteration 103, loss = 0.37795166\n",
      "Iteration 104, loss = 0.37756144\n",
      "Iteration 105, loss = 0.37711031\n",
      "Iteration 106, loss = 0.37665660\n",
      "Iteration 107, loss = 0.37638106\n",
      "Iteration 108, loss = 0.37592579\n",
      "Iteration 109, loss = 0.37547269\n",
      "Iteration 110, loss = 0.37506166\n",
      "Iteration 111, loss = 0.37465864\n",
      "Iteration 112, loss = 0.37430542\n",
      "Iteration 113, loss = 0.37390459\n",
      "Iteration 114, loss = 0.37365746\n",
      "Iteration 115, loss = 0.37312211\n",
      "Iteration 116, loss = 0.37284620\n",
      "Iteration 117, loss = 0.37245451\n",
      "Iteration 118, loss = 0.37206442\n",
      "Iteration 119, loss = 0.37169184\n",
      "Iteration 120, loss = 0.37142399\n",
      "Iteration 121, loss = 0.37109645\n",
      "Iteration 122, loss = 0.37068578\n",
      "Iteration 123, loss = 0.37040876\n",
      "Iteration 124, loss = 0.37003251\n",
      "Iteration 125, loss = 0.36973535\n",
      "Iteration 126, loss = 0.36944774\n",
      "Iteration 127, loss = 0.36924507\n",
      "Iteration 128, loss = 0.36876030\n",
      "Iteration 129, loss = 0.36852538\n",
      "Iteration 130, loss = 0.36817820\n",
      "Iteration 131, loss = 0.36787952\n",
      "Iteration 132, loss = 0.36756253\n",
      "Iteration 133, loss = 0.36724496\n",
      "Iteration 134, loss = 0.36695123\n",
      "Iteration 135, loss = 0.36671132\n",
      "Iteration 136, loss = 0.36643640\n",
      "Iteration 137, loss = 0.36605728\n",
      "Iteration 138, loss = 0.36585298\n",
      "Iteration 139, loss = 0.36555636\n",
      "Iteration 140, loss = 0.36523861\n",
      "Iteration 141, loss = 0.36502664\n",
      "Iteration 142, loss = 0.36465087\n",
      "Iteration 143, loss = 0.36441465\n",
      "Iteration 144, loss = 0.36411025\n",
      "Iteration 145, loss = 0.36383678\n",
      "Iteration 146, loss = 0.36355848\n",
      "Iteration 147, loss = 0.36336742\n",
      "Iteration 148, loss = 0.36314454\n",
      "Iteration 149, loss = 0.36290849\n",
      "Iteration 150, loss = 0.36272483\n",
      "Iteration 151, loss = 0.36235919\n",
      "Iteration 152, loss = 0.36211810\n",
      "Iteration 153, loss = 0.36193476\n",
      "Iteration 154, loss = 0.36160217\n",
      "Iteration 155, loss = 0.36148859\n",
      "Iteration 156, loss = 0.36111814\n",
      "Iteration 157, loss = 0.36088893\n",
      "Iteration 158, loss = 0.36069416\n",
      "Iteration 159, loss = 0.36045184\n",
      "Iteration 160, loss = 0.36029488\n",
      "Iteration 161, loss = 0.36007434\n",
      "Iteration 162, loss = 0.35974858\n",
      "Iteration 163, loss = 0.35948021\n",
      "Iteration 164, loss = 0.35932275\n",
      "Iteration 165, loss = 0.35916801\n",
      "Iteration 166, loss = 0.35884425\n",
      "Iteration 167, loss = 0.35858089\n",
      "Iteration 168, loss = 0.35847704\n",
      "Iteration 169, loss = 0.35824413\n",
      "Iteration 170, loss = 0.35803578\n",
      "Iteration 171, loss = 0.35778791\n",
      "Iteration 172, loss = 0.35752277\n",
      "Iteration 173, loss = 0.35744277\n",
      "Iteration 174, loss = 0.35720813\n",
      "Iteration 175, loss = 0.35705080\n",
      "Iteration 176, loss = 0.35677628\n",
      "Iteration 177, loss = 0.35659081\n",
      "Iteration 178, loss = 0.35642058\n",
      "Iteration 179, loss = 0.35616348\n",
      "Iteration 180, loss = 0.35599934\n",
      "Iteration 181, loss = 0.35577477\n",
      "Iteration 182, loss = 0.35567188\n",
      "Iteration 183, loss = 0.35538251\n",
      "Iteration 184, loss = 0.35515534\n",
      "Iteration 185, loss = 0.35507519\n",
      "Iteration 186, loss = 0.35487299\n",
      "Iteration 187, loss = 0.35460505\n",
      "Iteration 188, loss = 0.35452335\n",
      "Iteration 189, loss = 0.35417977\n",
      "Iteration 190, loss = 0.35410591\n",
      "Iteration 191, loss = 0.35392109\n",
      "Iteration 192, loss = 0.35373754\n",
      "Iteration 193, loss = 0.35364640\n",
      "Iteration 194, loss = 0.35333557\n",
      "Iteration 195, loss = 0.35315418\n",
      "Iteration 196, loss = 0.35294445\n",
      "Iteration 197, loss = 0.35305033\n",
      "Iteration 198, loss = 0.35263002\n",
      "Iteration 199, loss = 0.35247145\n",
      "Iteration 200, loss = 0.35230705\n",
      "Iteration 201, loss = 0.35210602\n",
      "Iteration 202, loss = 0.35192016\n",
      "Iteration 203, loss = 0.35180306\n",
      "Iteration 204, loss = 0.35164597\n",
      "Iteration 205, loss = 0.35144293\n",
      "Iteration 206, loss = 0.35122833\n",
      "Iteration 207, loss = 0.35117097\n",
      "Iteration 208, loss = 0.35101326\n",
      "Iteration 209, loss = 0.35072325\n",
      "Iteration 210, loss = 0.35071703\n",
      "Iteration 211, loss = 0.35047565\n",
      "Iteration 212, loss = 0.35033650\n",
      "Iteration 213, loss = 0.35017238\n",
      "Iteration 214, loss = 0.35000858\n",
      "Iteration 215, loss = 0.34986419\n",
      "Iteration 216, loss = 0.34970160\n",
      "Iteration 217, loss = 0.34946999\n",
      "Iteration 218, loss = 0.34950858\n",
      "Iteration 219, loss = 0.34920663\n",
      "Iteration 220, loss = 0.34905467\n",
      "Iteration 221, loss = 0.34892635\n",
      "Iteration 222, loss = 0.34867649\n",
      "Iteration 223, loss = 0.34862081\n",
      "Iteration 224, loss = 0.34838219\n",
      "Iteration 225, loss = 0.34823690\n",
      "Iteration 226, loss = 0.34820399\n",
      "Iteration 227, loss = 0.34806927\n",
      "Iteration 228, loss = 0.34791932\n",
      "Iteration 229, loss = 0.34778006\n",
      "Iteration 230, loss = 0.34752479\n",
      "Iteration 231, loss = 0.34743190\n",
      "Iteration 232, loss = 0.34714191\n",
      "Iteration 233, loss = 0.34708575\n",
      "Iteration 234, loss = 0.34705361\n",
      "Iteration 235, loss = 0.34687550\n",
      "Iteration 236, loss = 0.34673647\n",
      "Iteration 237, loss = 0.34647484\n",
      "Iteration 238, loss = 0.34641821\n",
      "Iteration 239, loss = 0.34639392\n",
      "Iteration 240, loss = 0.34616182\n",
      "Iteration 241, loss = 0.34597375\n",
      "Iteration 242, loss = 0.34580325\n",
      "Iteration 243, loss = 0.34563975\n",
      "Iteration 244, loss = 0.34556948\n",
      "Iteration 245, loss = 0.34540129\n",
      "Iteration 246, loss = 0.34514256\n",
      "Iteration 247, loss = 0.34516361\n",
      "Iteration 248, loss = 0.34488526\n",
      "Iteration 249, loss = 0.34474467\n",
      "Iteration 250, loss = 0.34462771\n",
      "Iteration 251, loss = 0.34452849\n",
      "Iteration 252, loss = 0.34443562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 253, loss = 0.34434310\n",
      "Iteration 254, loss = 0.34435096\n",
      "Iteration 255, loss = 0.34406153\n",
      "Iteration 256, loss = 0.34388789\n",
      "Iteration 257, loss = 0.34357196\n",
      "Iteration 258, loss = 0.34358871\n",
      "Iteration 259, loss = 0.34338748\n",
      "Iteration 260, loss = 0.34326641\n",
      "Iteration 261, loss = 0.34319252\n",
      "Iteration 262, loss = 0.34296142\n",
      "Iteration 263, loss = 0.34286405\n",
      "Iteration 264, loss = 0.34274685\n",
      "Iteration 265, loss = 0.34256726\n",
      "Iteration 266, loss = 0.34245527\n",
      "Iteration 267, loss = 0.34233333\n",
      "Iteration 268, loss = 0.34218643\n",
      "Iteration 269, loss = 0.34211210\n",
      "Iteration 270, loss = 0.34194641\n",
      "Iteration 271, loss = 0.34184808\n",
      "Iteration 272, loss = 0.34175666\n",
      "Iteration 273, loss = 0.34160728\n",
      "Iteration 274, loss = 0.34155050\n",
      "Iteration 275, loss = 0.34132620\n",
      "Iteration 276, loss = 0.34109975\n",
      "Iteration 277, loss = 0.34106100\n",
      "Iteration 278, loss = 0.34091701\n",
      "Iteration 279, loss = 0.34067276\n",
      "Iteration 280, loss = 0.34069264\n",
      "Iteration 281, loss = 0.34044068\n",
      "Iteration 282, loss = 0.34048505\n",
      "Iteration 283, loss = 0.34030088\n",
      "Iteration 284, loss = 0.34009897\n",
      "Iteration 285, loss = 0.33993888\n",
      "Iteration 286, loss = 0.33985595\n",
      "Iteration 287, loss = 0.33975588\n",
      "Iteration 288, loss = 0.33959565\n",
      "Iteration 289, loss = 0.33965926\n",
      "Iteration 290, loss = 0.33936810\n",
      "Iteration 291, loss = 0.33927315\n",
      "Iteration 292, loss = 0.33908253\n",
      "Iteration 293, loss = 0.33897582\n",
      "Iteration 294, loss = 0.33882329\n",
      "Iteration 295, loss = 0.33875849\n",
      "Iteration 296, loss = 0.33854746\n",
      "Iteration 297, loss = 0.33858459\n",
      "Iteration 298, loss = 0.33835825\n",
      "Iteration 299, loss = 0.33826984\n",
      "Iteration 300, loss = 0.33813770\n",
      "Iteration 301, loss = 0.33804960\n",
      "Iteration 302, loss = 0.33781566\n",
      "Iteration 303, loss = 0.33766753\n",
      "Iteration 304, loss = 0.33763912\n",
      "Iteration 305, loss = 0.33760146\n",
      "Iteration 306, loss = 0.33729210\n",
      "Iteration 307, loss = 0.33730108\n",
      "Iteration 308, loss = 0.33713595\n",
      "Iteration 309, loss = 0.33678322\n",
      "Iteration 310, loss = 0.33692818\n",
      "Iteration 311, loss = 0.33677295\n",
      "Iteration 312, loss = 0.33664880\n",
      "Iteration 313, loss = 0.33649633\n",
      "Iteration 314, loss = 0.33640800\n",
      "Iteration 315, loss = 0.33627530\n",
      "Iteration 316, loss = 0.33614414\n",
      "Iteration 317, loss = 0.33612078\n",
      "Iteration 318, loss = 0.33599072\n",
      "Iteration 319, loss = 0.33579615\n",
      "Iteration 320, loss = 0.33569333\n",
      "Iteration 321, loss = 0.33555356\n",
      "Iteration 322, loss = 0.33551179\n",
      "Iteration 323, loss = 0.33534713\n",
      "Iteration 324, loss = 0.33529685\n",
      "Iteration 325, loss = 0.33512436\n",
      "Iteration 326, loss = 0.33496753\n",
      "Iteration 327, loss = 0.33497669\n",
      "Iteration 328, loss = 0.33479203\n",
      "Iteration 329, loss = 0.33488469\n",
      "Iteration 330, loss = 0.33457098\n",
      "Iteration 331, loss = 0.33456024\n",
      "Iteration 332, loss = 0.33438295\n",
      "Iteration 333, loss = 0.33416175\n",
      "Iteration 334, loss = 0.33408316\n",
      "Iteration 335, loss = 0.33394623\n",
      "Iteration 336, loss = 0.33386571\n",
      "Iteration 337, loss = 0.33376234\n",
      "Iteration 338, loss = 0.33361233\n",
      "Iteration 339, loss = 0.33359130\n",
      "Iteration 340, loss = 0.33332716\n",
      "Iteration 341, loss = 0.33343032\n",
      "Iteration 342, loss = 0.33322021\n",
      "Iteration 343, loss = 0.33308116\n",
      "Iteration 344, loss = 0.33312695\n",
      "Iteration 345, loss = 0.33302514\n",
      "Iteration 346, loss = 0.33284172\n",
      "Iteration 347, loss = 0.33271294\n",
      "Iteration 348, loss = 0.33262335\n",
      "Iteration 349, loss = 0.33257039\n",
      "Iteration 350, loss = 0.33239555\n",
      "Iteration 351, loss = 0.33229510\n",
      "Iteration 352, loss = 0.33216286\n",
      "Iteration 353, loss = 0.33204991\n",
      "Iteration 354, loss = 0.33196592\n",
      "Iteration 355, loss = 0.33182145\n",
      "Iteration 356, loss = 0.33170037\n",
      "Iteration 357, loss = 0.33179259\n",
      "Iteration 358, loss = 0.33146515\n",
      "Iteration 359, loss = 0.33140411\n",
      "Iteration 360, loss = 0.33136342\n",
      "Iteration 361, loss = 0.33118442\n",
      "Iteration 362, loss = 0.33110765\n",
      "Iteration 363, loss = 0.33098092\n",
      "Iteration 364, loss = 0.33074086\n",
      "Iteration 365, loss = 0.33075838\n",
      "Iteration 366, loss = 0.33069251\n",
      "Iteration 367, loss = 0.33057054\n",
      "Iteration 368, loss = 0.33063358\n",
      "Iteration 369, loss = 0.33057025\n",
      "Iteration 370, loss = 0.33028266\n",
      "Iteration 371, loss = 0.33011176\n",
      "Iteration 372, loss = 0.33013679\n",
      "Iteration 373, loss = 0.33008197\n",
      "Iteration 374, loss = 0.32987405\n",
      "Iteration 375, loss = 0.32988568\n",
      "Iteration 376, loss = 0.32974299\n",
      "Iteration 377, loss = 0.32968161\n",
      "Iteration 378, loss = 0.32960048\n",
      "Iteration 379, loss = 0.32941153\n",
      "Iteration 380, loss = 0.32927681\n",
      "Iteration 381, loss = 0.32941077\n",
      "Iteration 382, loss = 0.32919229\n",
      "Iteration 383, loss = 0.32896107\n",
      "Iteration 384, loss = 0.32898023\n",
      "Iteration 385, loss = 0.32877581\n",
      "Iteration 386, loss = 0.32857475\n",
      "Iteration 387, loss = 0.32885625\n",
      "Iteration 388, loss = 0.32841658\n",
      "Iteration 389, loss = 0.32835307\n",
      "Iteration 390, loss = 0.32840499\n",
      "Iteration 391, loss = 0.32835897\n",
      "Iteration 392, loss = 0.32813099\n",
      "Iteration 393, loss = 0.32822928\n",
      "Iteration 394, loss = 0.32798577\n",
      "Iteration 395, loss = 0.32789563\n",
      "Iteration 396, loss = 0.32778445\n",
      "Iteration 397, loss = 0.32732822\n",
      "Iteration 398, loss = 0.32772838\n",
      "Iteration 399, loss = 0.32757522\n",
      "Iteration 400, loss = 0.32753461\n",
      "Iteration 401, loss = 0.32746903\n",
      "Iteration 402, loss = 0.32723772\n",
      "Iteration 403, loss = 0.32721604\n",
      "Iteration 404, loss = 0.32706317\n",
      "Iteration 405, loss = 0.32696826\n",
      "Iteration 406, loss = 0.32686659\n",
      "Iteration 407, loss = 0.32683226\n",
      "Iteration 408, loss = 0.32660335\n",
      "Iteration 409, loss = 0.32680491\n",
      "Iteration 410, loss = 0.32650914\n",
      "Iteration 411, loss = 0.32646548\n",
      "Iteration 412, loss = 0.32642014\n",
      "Iteration 413, loss = 0.32643838\n",
      "Iteration 414, loss = 0.32616989\n",
      "Iteration 415, loss = 0.32610529\n",
      "Iteration 416, loss = 0.32607452\n",
      "Iteration 417, loss = 0.32606784\n",
      "Iteration 418, loss = 0.32583768\n",
      "Iteration 419, loss = 0.32575486\n",
      "Iteration 420, loss = 0.32569055\n",
      "Iteration 421, loss = 0.32551071\n",
      "Iteration 422, loss = 0.32538190\n",
      "Iteration 423, loss = 0.32540722\n",
      "Iteration 424, loss = 0.32531257\n",
      "Iteration 425, loss = 0.32523653\n",
      "Iteration 426, loss = 0.32509318\n",
      "Iteration 427, loss = 0.32493247\n",
      "Iteration 428, loss = 0.32483040\n",
      "Iteration 429, loss = 0.32491694\n",
      "Iteration 430, loss = 0.32480230\n",
      "Iteration 431, loss = 0.32463440\n",
      "Iteration 432, loss = 0.32448984\n",
      "Iteration 433, loss = 0.32431626\n",
      "Iteration 434, loss = 0.32413011\n",
      "Iteration 435, loss = 0.32448461\n",
      "Iteration 436, loss = 0.32425617\n",
      "Iteration 437, loss = 0.32416017\n",
      "Iteration 438, loss = 0.32390945\n",
      "Iteration 439, loss = 0.32389899\n",
      "Iteration 440, loss = 0.32379947\n",
      "Iteration 441, loss = 0.32372426\n",
      "Iteration 442, loss = 0.32381218\n",
      "Iteration 443, loss = 0.32346785\n",
      "Iteration 444, loss = 0.32344237\n",
      "Iteration 445, loss = 0.32337349\n",
      "Iteration 446, loss = 0.32326513\n",
      "Iteration 447, loss = 0.32305308\n",
      "Iteration 448, loss = 0.32306074\n",
      "Iteration 449, loss = 0.32302555\n",
      "Iteration 450, loss = 0.32290865\n",
      "Iteration 451, loss = 0.32281978\n",
      "Iteration 452, loss = 0.32268276\n",
      "Iteration 453, loss = 0.32259447\n",
      "Iteration 454, loss = 0.32249984\n",
      "Iteration 455, loss = 0.32233191\n",
      "Iteration 456, loss = 0.32242630\n",
      "Iteration 457, loss = 0.32218664\n",
      "Iteration 458, loss = 0.32216279\n",
      "Iteration 459, loss = 0.32208185\n",
      "Iteration 460, loss = 0.32215689\n",
      "Iteration 461, loss = 0.32186783\n",
      "Iteration 462, loss = 0.32181963\n",
      "Iteration 463, loss = 0.32162070\n",
      "Iteration 464, loss = 0.32151504\n",
      "Iteration 465, loss = 0.32143456\n",
      "Iteration 466, loss = 0.32126444\n",
      "Iteration 467, loss = 0.32113187\n",
      "Iteration 468, loss = 0.32120143\n",
      "Iteration 469, loss = 0.32119350\n",
      "Iteration 470, loss = 0.32097968\n",
      "Iteration 471, loss = 0.32084028\n",
      "Iteration 472, loss = 0.32095848\n",
      "Iteration 473, loss = 0.32064427\n",
      "Iteration 474, loss = 0.32070915\n",
      "Iteration 475, loss = 0.32064207\n",
      "Iteration 476, loss = 0.32055716\n",
      "Iteration 477, loss = 0.32050052\n",
      "Iteration 478, loss = 0.32022943\n",
      "Iteration 479, loss = 0.32028170\n",
      "Iteration 480, loss = 0.32006077\n",
      "Iteration 481, loss = 0.31993491\n",
      "Iteration 482, loss = 0.32004193\n",
      "Iteration 483, loss = 0.31989297\n",
      "Iteration 484, loss = 0.31975403\n",
      "Iteration 485, loss = 0.31968755\n",
      "Iteration 486, loss = 0.31957751\n",
      "Iteration 487, loss = 0.31961177\n",
      "Iteration 488, loss = 0.31936117\n",
      "Iteration 489, loss = 0.31944985\n",
      "Iteration 490, loss = 0.31917398\n",
      "Iteration 491, loss = 0.31935748\n",
      "Iteration 492, loss = 0.31908376\n",
      "Iteration 493, loss = 0.31891140\n",
      "Iteration 494, loss = 0.31894439\n",
      "Iteration 495, loss = 0.31885869\n",
      "Iteration 496, loss = 0.31864254\n",
      "Iteration 497, loss = 0.31853557\n",
      "Iteration 498, loss = 0.31849101\n",
      "Iteration 499, loss = 0.31851388\n",
      "Iteration 500, loss = 0.31833778\n",
      "Iteration 501, loss = 0.31835665\n",
      "Iteration 502, loss = 0.31819351\n",
      "Iteration 503, loss = 0.31809615\n",
      "Iteration 504, loss = 0.31790146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 505, loss = 0.31782348\n",
      "Iteration 506, loss = 0.31781737\n",
      "Iteration 507, loss = 0.31757507\n",
      "Iteration 508, loss = 0.31765598\n",
      "Iteration 509, loss = 0.31752269\n",
      "Iteration 510, loss = 0.31739132\n",
      "Iteration 511, loss = 0.31733188\n",
      "Iteration 512, loss = 0.31734918\n",
      "Iteration 513, loss = 0.31711163\n",
      "Iteration 514, loss = 0.31698334\n",
      "Iteration 515, loss = 0.31708264\n",
      "Iteration 516, loss = 0.31680146\n",
      "Iteration 517, loss = 0.31673462\n",
      "Iteration 518, loss = 0.31672067\n",
      "Iteration 519, loss = 0.31664466\n",
      "Iteration 520, loss = 0.31642972\n",
      "Iteration 521, loss = 0.31650935\n",
      "Iteration 522, loss = 0.31643403\n",
      "Iteration 523, loss = 0.31628770\n",
      "Iteration 524, loss = 0.31605694\n",
      "Iteration 525, loss = 0.31593239\n",
      "Iteration 526, loss = 0.31589676\n",
      "Iteration 527, loss = 0.31592974\n",
      "Iteration 528, loss = 0.31576035\n",
      "Iteration 529, loss = 0.31561472\n",
      "Iteration 530, loss = 0.31571037\n",
      "Iteration 531, loss = 0.31547040\n",
      "Iteration 532, loss = 0.31527773\n",
      "Iteration 533, loss = 0.31522106\n",
      "Iteration 534, loss = 0.31530102\n",
      "Iteration 535, loss = 0.31510117\n",
      "Iteration 536, loss = 0.31513120\n",
      "Iteration 537, loss = 0.31487314\n",
      "Iteration 538, loss = 0.31457009\n",
      "Iteration 539, loss = 0.31461277\n",
      "Iteration 540, loss = 0.31467276\n",
      "Iteration 541, loss = 0.31452634\n",
      "Iteration 542, loss = 0.31465275\n",
      "Iteration 543, loss = 0.31432330\n",
      "Iteration 544, loss = 0.31413835\n",
      "Iteration 545, loss = 0.31412490\n",
      "Iteration 546, loss = 0.31416151\n",
      "Iteration 547, loss = 0.31403979\n",
      "Iteration 548, loss = 0.31374656\n",
      "Iteration 549, loss = 0.31373301\n",
      "Iteration 550, loss = 0.31357362\n",
      "Iteration 551, loss = 0.31350021\n",
      "Iteration 552, loss = 0.31353174\n",
      "Iteration 553, loss = 0.31340215\n",
      "Iteration 554, loss = 0.31319545\n",
      "Iteration 555, loss = 0.31334596\n",
      "Iteration 556, loss = 0.31304411\n",
      "Iteration 557, loss = 0.31313508\n",
      "Iteration 558, loss = 0.31291000\n",
      "Iteration 559, loss = 0.31271441\n",
      "Iteration 560, loss = 0.31254749\n",
      "Iteration 561, loss = 0.31265326\n",
      "Iteration 562, loss = 0.31264917\n",
      "Iteration 563, loss = 0.31260443\n",
      "Iteration 564, loss = 0.31225939\n",
      "Iteration 565, loss = 0.31219365\n",
      "Iteration 566, loss = 0.31207597\n",
      "Iteration 567, loss = 0.31195995\n",
      "Iteration 568, loss = 0.31172239\n",
      "Iteration 569, loss = 0.31192086\n",
      "Iteration 570, loss = 0.31169165\n",
      "Iteration 571, loss = 0.31157975\n",
      "Iteration 572, loss = 0.31155236\n",
      "Iteration 573, loss = 0.31140784\n",
      "Iteration 574, loss = 0.31129018\n",
      "Iteration 575, loss = 0.31127566\n",
      "Iteration 576, loss = 0.31099898\n",
      "Iteration 577, loss = 0.31096388\n",
      "Iteration 578, loss = 0.31099950\n",
      "Iteration 579, loss = 0.31086519\n",
      "Iteration 580, loss = 0.31072195\n",
      "Iteration 581, loss = 0.31066655\n",
      "Iteration 582, loss = 0.31058859\n",
      "Iteration 583, loss = 0.31049682\n",
      "Iteration 584, loss = 0.31044180\n",
      "Iteration 585, loss = 0.31022767\n",
      "Iteration 586, loss = 0.31029953\n",
      "Iteration 587, loss = 0.31013796\n",
      "Iteration 588, loss = 0.31011509\n",
      "Iteration 589, loss = 0.31015993\n",
      "Iteration 590, loss = 0.30998102\n",
      "Iteration 591, loss = 0.30976556\n",
      "Iteration 592, loss = 0.30969185\n",
      "Iteration 593, loss = 0.30954763\n",
      "Iteration 594, loss = 0.30949670\n",
      "Iteration 595, loss = 0.30936368\n",
      "Iteration 596, loss = 0.30937407\n",
      "Iteration 597, loss = 0.30923944\n",
      "Iteration 598, loss = 0.30910181\n",
      "Iteration 599, loss = 0.30896983\n",
      "Iteration 600, loss = 0.30902150\n",
      "Iteration 601, loss = 0.30884467\n",
      "Iteration 602, loss = 0.30885341\n",
      "Iteration 603, loss = 0.30871988\n",
      "Iteration 604, loss = 0.30858686\n",
      "Iteration 605, loss = 0.30841256\n",
      "Iteration 606, loss = 0.30843597\n",
      "Iteration 607, loss = 0.30812922\n",
      "Iteration 608, loss = 0.30821573\n",
      "Iteration 609, loss = 0.30802820\n",
      "Iteration 610, loss = 0.30799755\n",
      "Iteration 611, loss = 0.30789916\n",
      "Iteration 612, loss = 0.30780403\n",
      "Iteration 613, loss = 0.30786066\n",
      "Iteration 614, loss = 0.30770409\n",
      "Iteration 615, loss = 0.30752993\n",
      "Iteration 616, loss = 0.30755585\n",
      "Iteration 617, loss = 0.30748949\n",
      "Iteration 618, loss = 0.30743234\n",
      "Iteration 619, loss = 0.30716556\n",
      "Iteration 620, loss = 0.30725454\n",
      "Iteration 621, loss = 0.30747563\n",
      "Iteration 622, loss = 0.30708772\n",
      "Iteration 623, loss = 0.30696740\n",
      "Iteration 624, loss = 0.30676534\n",
      "Iteration 625, loss = 0.30670265\n",
      "Iteration 626, loss = 0.30664781\n",
      "Iteration 627, loss = 0.30658632\n",
      "Iteration 628, loss = 0.30654516\n",
      "Iteration 629, loss = 0.30642437\n",
      "Iteration 630, loss = 0.30632927\n",
      "Iteration 631, loss = 0.30617834\n",
      "Iteration 632, loss = 0.30608682\n",
      "Iteration 633, loss = 0.30625842\n",
      "Iteration 634, loss = 0.30604820\n",
      "Iteration 635, loss = 0.30582744\n",
      "Iteration 636, loss = 0.30580877\n",
      "Iteration 637, loss = 0.30581092\n",
      "Iteration 638, loss = 0.30560608\n",
      "Iteration 639, loss = 0.30548258\n",
      "Iteration 640, loss = 0.30539084\n",
      "Iteration 641, loss = 0.30530178\n",
      "Iteration 642, loss = 0.30520716\n",
      "Iteration 643, loss = 0.30521509\n",
      "Iteration 644, loss = 0.30526930\n",
      "Iteration 645, loss = 0.30526905\n",
      "Iteration 646, loss = 0.30506671\n",
      "Iteration 647, loss = 0.30489454\n",
      "Iteration 648, loss = 0.30476839\n",
      "Iteration 649, loss = 0.30478966\n",
      "Iteration 650, loss = 0.30471802\n",
      "Iteration 651, loss = 0.30454283\n",
      "Iteration 652, loss = 0.30441391\n",
      "Iteration 653, loss = 0.30443078\n",
      "Iteration 654, loss = 0.30432119\n",
      "Iteration 655, loss = 0.30410004\n",
      "Iteration 656, loss = 0.30405786\n",
      "Iteration 657, loss = 0.30406883\n",
      "Iteration 658, loss = 0.30413699\n",
      "Iteration 659, loss = 0.30382705\n",
      "Iteration 660, loss = 0.30384075\n",
      "Iteration 661, loss = 0.30387298\n",
      "Iteration 662, loss = 0.30364145\n",
      "Iteration 663, loss = 0.30371097\n",
      "Iteration 664, loss = 0.30360100\n",
      "Iteration 665, loss = 0.30330038\n",
      "Iteration 666, loss = 0.30349865\n",
      "Iteration 667, loss = 0.30323106\n",
      "Iteration 668, loss = 0.30318155\n",
      "Iteration 669, loss = 0.30306270\n",
      "Iteration 670, loss = 0.30307944\n",
      "Iteration 671, loss = 0.30298635\n",
      "Iteration 672, loss = 0.30287243\n",
      "Iteration 673, loss = 0.30276371\n",
      "Iteration 674, loss = 0.30280671\n",
      "Iteration 675, loss = 0.30281255\n",
      "Iteration 676, loss = 0.30268552\n",
      "Iteration 677, loss = 0.30256284\n",
      "Iteration 678, loss = 0.30241163\n",
      "Iteration 679, loss = 0.30233012\n",
      "Iteration 680, loss = 0.30229371\n",
      "Iteration 681, loss = 0.30230089\n",
      "Iteration 682, loss = 0.30209661\n",
      "Iteration 683, loss = 0.30204661\n",
      "Iteration 684, loss = 0.30192632\n",
      "Iteration 685, loss = 0.30183670\n",
      "Iteration 686, loss = 0.30186812\n",
      "Iteration 687, loss = 0.30163075\n",
      "Iteration 688, loss = 0.30163315\n",
      "Iteration 689, loss = 0.30170239\n",
      "Iteration 690, loss = 0.30138647\n",
      "Iteration 691, loss = 0.30139399\n",
      "Iteration 692, loss = 0.30133328\n",
      "Iteration 693, loss = 0.30115762\n",
      "Iteration 694, loss = 0.30140566\n",
      "Iteration 695, loss = 0.30116277\n",
      "Iteration 696, loss = 0.30101211\n",
      "Iteration 697, loss = 0.30091783\n",
      "Iteration 698, loss = 0.30103966\n",
      "Iteration 699, loss = 0.30094583\n",
      "Iteration 700, loss = 0.30079103\n",
      "Iteration 701, loss = 0.30086486\n",
      "Iteration 702, loss = 0.30085357\n",
      "Iteration 703, loss = 0.30038586\n",
      "Iteration 704, loss = 0.30040814\n",
      "Iteration 705, loss = 0.30046938\n",
      "Iteration 706, loss = 0.30018806\n",
      "Iteration 707, loss = 0.30024713\n",
      "Iteration 708, loss = 0.30011181\n",
      "Iteration 709, loss = 0.30024180\n",
      "Iteration 710, loss = 0.30011984\n",
      "Iteration 711, loss = 0.29992913\n",
      "Iteration 712, loss = 0.29982736\n",
      "Iteration 713, loss = 0.29995848\n",
      "Iteration 714, loss = 0.29963036\n",
      "Iteration 715, loss = 0.29963608\n",
      "Iteration 716, loss = 0.29956708\n",
      "Iteration 717, loss = 0.29949724\n",
      "Iteration 718, loss = 0.29951770\n",
      "Iteration 719, loss = 0.29947649\n",
      "Iteration 720, loss = 0.29925466\n",
      "Iteration 721, loss = 0.29919899\n",
      "Iteration 722, loss = 0.29910960\n",
      "Iteration 723, loss = 0.29898507\n",
      "Iteration 724, loss = 0.29897090\n",
      "Iteration 725, loss = 0.29902443\n",
      "Iteration 726, loss = 0.29887477\n",
      "Iteration 727, loss = 0.29889272\n",
      "Iteration 728, loss = 0.29878351\n",
      "Iteration 729, loss = 0.29868455\n",
      "Iteration 730, loss = 0.29859246\n",
      "Iteration 731, loss = 0.29841473\n",
      "Iteration 732, loss = 0.29853719\n",
      "Iteration 733, loss = 0.29828316\n",
      "Iteration 734, loss = 0.29835276\n",
      "Iteration 735, loss = 0.29827405\n",
      "Iteration 736, loss = 0.29817256\n",
      "Iteration 737, loss = 0.29807897\n",
      "Iteration 738, loss = 0.29786769\n",
      "Iteration 739, loss = 0.29816333\n",
      "Iteration 740, loss = 0.29779558\n",
      "Iteration 741, loss = 0.29778460\n",
      "Iteration 742, loss = 0.29769388\n",
      "Iteration 743, loss = 0.29746095\n",
      "Iteration 744, loss = 0.29757522\n",
      "Iteration 745, loss = 0.29759211\n",
      "Iteration 746, loss = 0.29743999\n",
      "Iteration 747, loss = 0.29732420\n",
      "Iteration 748, loss = 0.29726387\n",
      "Iteration 749, loss = 0.29714789\n",
      "Iteration 750, loss = 0.29722307\n",
      "Iteration 751, loss = 0.29700747\n",
      "Iteration 752, loss = 0.29709722\n",
      "Iteration 753, loss = 0.29693810\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 754, loss = 0.29700371\n",
      "Iteration 755, loss = 0.29681483\n",
      "Iteration 756, loss = 0.29661844\n",
      "Iteration 757, loss = 0.29668550\n",
      "Iteration 758, loss = 0.29675032\n",
      "Iteration 759, loss = 0.29653593\n",
      "Iteration 760, loss = 0.29642167\n",
      "Iteration 761, loss = 0.29637460\n",
      "Iteration 762, loss = 0.29643078\n",
      "Iteration 763, loss = 0.29619640\n",
      "Iteration 764, loss = 0.29616594\n",
      "Iteration 765, loss = 0.29612347\n",
      "Iteration 766, loss = 0.29611742\n",
      "Iteration 767, loss = 0.29612429\n",
      "Iteration 768, loss = 0.29605529\n",
      "Iteration 769, loss = 0.29574772\n",
      "Iteration 770, loss = 0.29583625\n",
      "Iteration 771, loss = 0.29603240\n",
      "Iteration 772, loss = 0.29580742\n",
      "Iteration 773, loss = 0.29570243\n",
      "Iteration 774, loss = 0.29551710\n",
      "Iteration 775, loss = 0.29538823\n",
      "Iteration 776, loss = 0.29561968\n",
      "Iteration 777, loss = 0.29543038\n",
      "Iteration 778, loss = 0.29512156\n",
      "Iteration 779, loss = 0.29551612\n",
      "Iteration 780, loss = 0.29501093\n",
      "Iteration 781, loss = 0.29523136\n",
      "Iteration 782, loss = 0.29526747\n",
      "Iteration 783, loss = 0.29485417\n",
      "Iteration 784, loss = 0.29482758\n",
      "Iteration 785, loss = 0.29514626\n",
      "Iteration 786, loss = 0.29488809\n",
      "Iteration 787, loss = 0.29480012\n",
      "Iteration 788, loss = 0.29484412\n",
      "Iteration 789, loss = 0.29482565\n",
      "Iteration 790, loss = 0.29462488\n",
      "Iteration 791, loss = 0.29463558\n",
      "Iteration 792, loss = 0.29443778\n",
      "Iteration 793, loss = 0.29447555\n",
      "Iteration 794, loss = 0.29440484\n",
      "Iteration 795, loss = 0.29417775\n",
      "Iteration 796, loss = 0.29436852\n",
      "Iteration 797, loss = 0.29434424\n",
      "Iteration 798, loss = 0.29403186\n",
      "Iteration 799, loss = 0.29414959\n",
      "Iteration 800, loss = 0.29393903\n",
      "Iteration 801, loss = 0.29406858\n",
      "Iteration 802, loss = 0.29383760\n",
      "Iteration 803, loss = 0.29381385\n",
      "Iteration 804, loss = 0.29380237\n",
      "Iteration 805, loss = 0.29385644\n",
      "Iteration 806, loss = 0.29375901\n",
      "Iteration 807, loss = 0.29357412\n",
      "Iteration 808, loss = 0.29346095\n",
      "Iteration 809, loss = 0.29354049\n",
      "Iteration 810, loss = 0.29350377\n",
      "Iteration 811, loss = 0.29349442\n",
      "Iteration 812, loss = 0.29348090\n",
      "Iteration 813, loss = 0.29330784\n",
      "Iteration 814, loss = 0.29327973\n",
      "Iteration 815, loss = 0.29325101\n",
      "Iteration 816, loss = 0.29325208\n",
      "Iteration 817, loss = 0.29314016\n",
      "Iteration 818, loss = 0.29294857\n",
      "Iteration 819, loss = 0.29289114\n",
      "Iteration 820, loss = 0.29306248\n",
      "Iteration 821, loss = 0.29290464\n",
      "Iteration 822, loss = 0.29278305\n",
      "Iteration 823, loss = 0.29273904\n",
      "Iteration 824, loss = 0.29284328\n",
      "Iteration 825, loss = 0.29254305\n",
      "Iteration 826, loss = 0.29251330\n",
      "Iteration 827, loss = 0.29254990\n",
      "Iteration 828, loss = 0.29242944\n",
      "Iteration 829, loss = 0.29243153\n",
      "Iteration 830, loss = 0.29244357\n",
      "Iteration 831, loss = 0.29222302\n",
      "Iteration 832, loss = 0.29244563\n",
      "Iteration 833, loss = 0.29208456\n",
      "Iteration 834, loss = 0.29230289\n",
      "Iteration 835, loss = 0.29196180\n",
      "Iteration 836, loss = 0.29215503\n",
      "Iteration 837, loss = 0.29205342\n",
      "Iteration 838, loss = 0.29181814\n",
      "Iteration 839, loss = 0.29182136\n",
      "Iteration 840, loss = 0.29183630\n",
      "Iteration 841, loss = 0.29168047\n",
      "Iteration 842, loss = 0.29171588\n",
      "Iteration 843, loss = 0.29158197\n",
      "Iteration 844, loss = 0.29167428\n",
      "Iteration 845, loss = 0.29159525\n",
      "Iteration 846, loss = 0.29147719\n",
      "Iteration 847, loss = 0.29168269\n",
      "Iteration 848, loss = 0.29150348\n",
      "Iteration 849, loss = 0.29133945\n",
      "Iteration 850, loss = 0.29147231\n",
      "Iteration 851, loss = 0.29132861\n",
      "Iteration 852, loss = 0.29115915\n",
      "Iteration 853, loss = 0.29102664\n",
      "Iteration 854, loss = 0.29107287\n",
      "Iteration 855, loss = 0.29105690\n",
      "Iteration 856, loss = 0.29117212\n",
      "Iteration 857, loss = 0.29092167\n",
      "Iteration 858, loss = 0.29092938\n",
      "Iteration 859, loss = 0.29090185\n",
      "Iteration 860, loss = 0.29082213\n",
      "Iteration 861, loss = 0.29075554\n",
      "Iteration 862, loss = 0.29094036\n",
      "Iteration 863, loss = 0.29062144\n",
      "Iteration 864, loss = 0.29062326\n",
      "Iteration 865, loss = 0.29049241\n",
      "Iteration 866, loss = 0.29064916\n",
      "Iteration 867, loss = 0.29052659\n",
      "Iteration 868, loss = 0.29034369\n",
      "Iteration 869, loss = 0.29066665\n",
      "Iteration 870, loss = 0.29043363\n",
      "Iteration 871, loss = 0.29009038\n",
      "Iteration 872, loss = 0.29023351\n",
      "Iteration 873, loss = 0.29017070\n",
      "Iteration 874, loss = 0.29020563\n",
      "Iteration 875, loss = 0.29013553\n",
      "Iteration 876, loss = 0.29051602\n",
      "Iteration 877, loss = 0.28996517\n",
      "Iteration 878, loss = 0.29002619\n",
      "Iteration 879, loss = 0.29011939\n",
      "Iteration 880, loss = 0.28977103\n",
      "Iteration 881, loss = 0.28994792\n",
      "Iteration 882, loss = 0.28978710\n",
      "Iteration 883, loss = 0.28976272\n",
      "Iteration 884, loss = 0.28975533\n",
      "Iteration 885, loss = 0.28966323\n",
      "Iteration 886, loss = 0.28953722\n",
      "Iteration 887, loss = 0.28968633\n",
      "Iteration 888, loss = 0.28948234\n",
      "Iteration 889, loss = 0.28958255\n",
      "Iteration 890, loss = 0.28941442\n",
      "Iteration 891, loss = 0.28917102\n",
      "Iteration 892, loss = 0.28960669\n",
      "Iteration 893, loss = 0.28942465\n",
      "Iteration 894, loss = 0.28927596\n",
      "Iteration 895, loss = 0.28916704\n",
      "Iteration 896, loss = 0.28945205\n",
      "Iteration 897, loss = 0.28922812\n",
      "Iteration 898, loss = 0.28917690\n",
      "Iteration 899, loss = 0.28896876\n",
      "Iteration 900, loss = 0.28890241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anujjoshi/miniconda3/envs/uni_project/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (900) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8745517288043846\n",
      "Precision: Class A 0.82\n",
      "Precision: Class B 0.89\n",
      "Recall: Class A 0.71\n",
      "Recall: Class B 0.94\n",
      "F1-Score: Class A 0.76\n",
      "F1-Score: Class B 0.92\n",
      "Average F1-score: 0.84\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "clf = MLPClassifier(solver='sgd', alpha=1e-5,hidden_layer_sizes=(10), max_iter=1000, verbose=True)\n",
    "clf.fit(normalized_X_Human_Non_Human_train, y_train)\n",
    "result = clf.predict(normalized_X_Human_Non_Human_test)\n",
    "create_confusion_matrix(y_test, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anujjoshi/miniconda3/envs/uni_project/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/anujjoshi/miniconda3/envs/uni_project/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/anujjoshi/miniconda3/envs/uni_project/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden layer 10 score average 0.8337620000905929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anujjoshi/miniconda3/envs/uni_project/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/anujjoshi/miniconda3/envs/uni_project/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/anujjoshi/miniconda3/envs/uni_project/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden layer 11 score average 0.8331302780103437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anujjoshi/miniconda3/envs/uni_project/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/anujjoshi/miniconda3/envs/uni_project/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/anujjoshi/miniconda3/envs/uni_project/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden layer 12 score average 0.8335814112583817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anujjoshi/miniconda3/envs/uni_project/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/anujjoshi/miniconda3/envs/uni_project/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/anujjoshi/miniconda3/envs/uni_project/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden layer 13 score average 0.8342580867125206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anujjoshi/miniconda3/envs/uni_project/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/anujjoshi/miniconda3/envs/uni_project/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/anujjoshi/miniconda3/envs/uni_project/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden layer 14 score average 0.8361528378123054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anujjoshi/miniconda3/envs/uni_project/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/anujjoshi/miniconda3/envs/uni_project/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/anujjoshi/miniconda3/envs/uni_project/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden layer 15 score average 0.834664236072534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anujjoshi/miniconda3/envs/uni_project/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/anujjoshi/miniconda3/envs/uni_project/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n",
      "/Users/anujjoshi/miniconda3/envs/uni_project/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden layer 16 score average 0.826136821190306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anujjoshi/miniconda3/envs/uni_project/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n",
      "/Users/anujjoshi/miniconda3/envs/uni_project/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/anujjoshi/miniconda3/envs/uni_project/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden layer 17 score average 0.7970004642935242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anujjoshi/miniconda3/envs/uni_project/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/anujjoshi/miniconda3/envs/uni_project/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/anujjoshi/miniconda3/envs/uni_project/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden layer 18 score average 0.8380024401816358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anujjoshi/miniconda3/envs/uni_project/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/anujjoshi/miniconda3/envs/uni_project/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/anujjoshi/miniconda3/envs/uni_project/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden layer 19 score average 0.8358370591867855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anujjoshi/miniconda3/envs/uni_project/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/anujjoshi/miniconda3/envs/uni_project/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/anujjoshi/miniconda3/envs/uni_project/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden layer 20 score average 0.8348896866850016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anujjoshi/miniconda3/envs/uni_project/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/anujjoshi/miniconda3/envs/uni_project/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/anujjoshi/miniconda3/envs/uni_project/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden layer 21 score average 0.8332204716736178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anujjoshi/miniconda3/envs/uni_project/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/anujjoshi/miniconda3/envs/uni_project/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/anujjoshi/miniconda3/envs/uni_project/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden layer 22 score average 0.8337618718799978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anujjoshi/miniconda3/envs/uni_project/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/anujjoshi/miniconda3/envs/uni_project/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/anujjoshi/miniconda3/envs/uni_project/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden layer 23 score average 0.8346190018644934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anujjoshi/miniconda3/envs/uni_project/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/anujjoshi/miniconda3/envs/uni_project/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n",
      "/Users/anujjoshi/miniconda3/envs/uni_project/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden layer 24 score average 0.7953664097802635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anujjoshi/miniconda3/envs/uni_project/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n",
      "/Users/anujjoshi/miniconda3/envs/uni_project/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-92-c279003842ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m90\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMLPClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msolver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sgd'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhidden_layer_sizes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"hidden layer\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"score average\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/uni_project/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    400\u001b[0m                                 \u001b[0mfit_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m                                 \u001b[0mpre_dispatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m                                 error_score=error_score)\n\u001b[0m\u001b[1;32m    403\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/uni_project/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    238\u001b[0m             \u001b[0mreturn_times\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_estimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_estimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m             error_score=error_score)\n\u001b[0;32m--> 240\u001b[0;31m         for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0mzipped_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/uni_project/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    918\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 920\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    921\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/uni_project/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    757\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    760\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/uni_project/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    714\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 716\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    717\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/uni_project/lib/python3.7/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/uni_project/lib/python3.7/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/uni_project/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 225\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/uni_project/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 225\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/uni_project/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    526\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/uni_project/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    975\u001b[0m         \"\"\"\n\u001b[1;32m    976\u001b[0m         return self._fit(X, y, incremental=(self.warm_start and\n\u001b[0;32m--> 977\u001b[0;31m                                             hasattr(self, \"classes_\")))\n\u001b[0m\u001b[1;32m    978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/uni_project/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, incremental)\u001b[0m\n\u001b[1;32m    322\u001b[0m                              hidden_layer_sizes)\n\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincremental\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/uni_project/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py\u001b[0m in \u001b[0;36m_validate_input\u001b[0;34m(self, X, y, incremental)\u001b[0m\n\u001b[1;32m    912\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_validate_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincremental\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m         X, y = check_X_y(X, y, accept_sparse=['csr', 'csc', 'coo'],\n\u001b[0;32m--> 914\u001b[0;31m                          multi_output=True)\n\u001b[0m\u001b[1;32m    915\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/uni_project/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    756\u001b[0m                     \u001b[0mensure_min_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_min_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m                     \u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 758\u001b[0;31m                     estimator=estimator)\n\u001b[0m\u001b[1;32m    759\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[0;32m~/miniconda3/envs/uni_project/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    525\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 527\u001b[0;31m                 \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    528\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[0;32m~/miniconda3/envs/uni_project/lib/python3.7/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m     \"\"\"\n\u001b[0;32m--> 501\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(10,90):\n",
    "    clf = MLPClassifier(solver='sgd', alpha=1e-5,hidden_layer_sizes=(i), random_state=12)\n",
    "    scores = cross_val_score(clf, X_train, y_train, cv=3, scoring=\"accuracy\")\n",
    "    print(\"hidden layer\", i, \"score average\", scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9408760253830676\n",
      "Precision: Class A 0.94\n",
      "Precision: Class B 0.94\n",
      "Recall: Class A 0.89\n",
      "Recall: Class B 0.97\n",
      "F1-Score: Class A 0.91\n",
      "F1-Score: Class B 0.95\n",
      "Average F1-score: 0.93\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "clf = MLPClassifier(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(15), random_state=12)\n",
    "clf.fit(normalized_X_Human_Non_Human_train, y_train)\n",
    "result = clf.predict(normalized_X_Human_Non_Human_test)\n",
    "create_confusion_matrix(y_test, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anujjoshi/miniconda3/envs/uni_project/lib/python3.7/site-packages/sklearn/externals/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "/Users/anujjoshi/miniconda3/envs/uni_project/lib/python3.7/site-packages/sklearn/externals/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "/Users/anujjoshi/miniconda3/envs/uni_project/lib/python3.7/site-packages/sklearn/externals/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "/Users/anujjoshi/miniconda3/envs/uni_project/lib/python3.7/site-packages/sklearn/externals/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "/Users/anujjoshi/miniconda3/envs/uni_project/lib/python3.7/site-packages/sklearn/externals/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "/Users/anujjoshi/miniconda3/envs/uni_project/lib/python3.7/site-packages/sklearn/externals/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "/Users/anujjoshi/miniconda3/envs/uni_project/lib/python3.7/site-packages/sklearn/externals/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "/Users/anujjoshi/miniconda3/envs/uni_project/lib/python3.7/site-packages/sklearn/externals/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "/Users/anujjoshi/miniconda3/envs/uni_project/lib/python3.7/site-packages/sklearn/externals/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "/Users/anujjoshi/miniconda3/envs/uni_project/lib/python3.7/site-packages/sklearn/externals/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "/Users/anujjoshi/miniconda3/envs/uni_project/lib/python3.7/site-packages/sklearn/externals/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "/Users/anujjoshi/miniconda3/envs/uni_project/lib/python3.7/site-packages/sklearn/externals/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "/Users/anujjoshi/miniconda3/envs/uni_project/lib/python3.7/site-packages/sklearn/externals/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "/Users/anujjoshi/miniconda3/envs/uni_project/lib/python3.7/site-packages/sklearn/externals/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "/Users/anujjoshi/miniconda3/envs/uni_project/lib/python3.7/site-packages/sklearn/externals/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    }
   ],
   "source": [
    "hidden_layer_sizes = []\n",
    "for i in range(5,90):\n",
    "    hidden_layer_sizes.append(i)\n",
    "param_grid = [\n",
    "        {\n",
    "            'activation' : ['tanh'],\n",
    "            'solver' : ['lbfgs','sgd'],\n",
    "            'hidden_layer_sizes': hidden_layer_sizes,\n",
    "        }\n",
    "       ]\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "clf = GridSearchCV(MLPClassifier(max_iter=1000, warm_start=True), param_grid, cv=3,scoring='accuracy',n_jobs=-1)\n",
    "clf.fit(X_normalized,y)\n",
    "\n",
    "\n",
    "print(\"Best parameters set found on development set:\")\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
